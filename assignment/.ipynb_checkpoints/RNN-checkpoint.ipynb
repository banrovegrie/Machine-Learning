{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y45yhDufI5Uo"
   },
   "source": [
    "# **RNN**\n",
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55i8db8uI5U3"
   },
   "source": [
    "IMDB sentiment classification task\n",
    "\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.\n",
    "\n",
    "You can download the dataset from http://ai.stanford.edu/~amaas/data/sentiment/  or you can directly use \n",
    "\" from keras.datasets import imdb \" to import the dataset.\n",
    "\n",
    "Few points to be noted:\n",
    "Modules like SimpleRNN, LSTM, Activation layers, Dense layers, Dropout can be directly used from keras\n",
    "For preprocessing, you can use required "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SozhvLNkI5U6",
    "outputId": "4b1205f3-463f-430e-bf4c-76b51c290993"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "#load the imdb dataset \n",
    "from keras.datasets import imdb\n",
    "\n",
    "vocabulary_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
    "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NrilwfurI5VA",
    "outputId": "993b9c21-dc39-451f-e753-5f96b3e6ea1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---review---\n",
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "---label---\n",
      "1\n",
      "---review with words---\n",
      "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#the review is stored as a sequence of integers. \n",
    "# These are word IDs that have been pre-assigned to individual words, and the label is an integer\n",
    "\n",
    "print('---review---')\n",
    "print(X_train[0])\n",
    "print('---label---')\n",
    "print(y_train[0])\n",
    "\n",
    "# to get the actual review\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[0]])\n",
    "print('---label---')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "h6upWxEWI5VC"
   },
   "outputs": [],
   "source": [
    "#pad sequences (write your code here)\n",
    "from keras.preprocessing import sequence\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = 500)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RcCOpeNI5VF",
    "outputId": "e142fb7f-3817-4688-e31b-749eb90328c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 100)               13300     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 173,401\n",
      "Trainable params: 173,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#design a RNN model (write your code)\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, SimpleRNN\n",
    "\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(vocabulary_size, 32, input_length = 500))\n",
    "rnn.add(SimpleRNN(100, dropout = 0.2))\n",
    "rnn.add(Dense(1, activation='sigmoid'))\n",
    "print(rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "InQ2TED3I5VI"
   },
   "outputs": [],
   "source": [
    "#train and evaluate your model\n",
    "#choose your loss function and optimizer and mention the reason to choose that particular loss function and optimizer\n",
    "# use accuracy as the evaluation metric\n",
    "\n",
    "rnn.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e47CLjxgGpK8"
   },
   "source": [
    "Adam seemed to work better when compared to SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6A9Q0xmI5VJ",
    "outputId": "531f3294-422a-4b0b-e164-caf3682c51ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 22s 22s/step - loss: 0.6919 - accuracy: 0.5469 - val_loss: 0.7021 - val_accuracy: 0.5032\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 21s 21s/step - loss: 0.6192 - accuracy: 0.7031 - val_loss: 0.7002 - val_accuracy: 0.5061\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 21s 21s/step - loss: 0.5482 - accuracy: 0.8906 - val_loss: 0.7033 - val_accuracy: 0.5094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40fa0c96d0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "x1 = X_train[:batch_size]\n",
    "y1 = y_train[:batch_size]\n",
    "x_valid = X_train[batch_size:]\n",
    "y_valid = y_train[batch_size:]\n",
    "rnn.fit(\n",
    "    x1, y1,\n",
    "    validation_data = (x_valid, y_valid),\n",
    "    batch_size = batch_size,\n",
    "    epochs = num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YTXG__EmI5VM",
    "outputId": "00c1fa1a-af1b-4694-b3cb-82bd0c360101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (SimpleRNN):  0.5072799921035767\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model using model.evaluate()\n",
    "scores = rnn.evaluate(X_test, y_test, verbose = 0)\n",
    "print('Accuracy (SimpleRNN): ', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1uSo8DgI5VN"
   },
   "source": [
    "# **LSTM**\n",
    "\n",
    "Instead of using a RNN, now try using a LSTM model and compare both of them. Which of those performed better and why ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bk4rLYHwI5VP",
    "outputId": "8aafaa7b-004a-4d97-bf2c-e66580e150d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(vocabulary_size, 32, input_length = 500))\n",
    "lstm.add(LSTM(100))\n",
    "lstm.add(Dense(1, activation = 'sigmoid'))\n",
    "print(lstm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "3oXMLdUkGkvR"
   },
   "outputs": [],
   "source": [
    "lstm.compile(\n",
    "    loss = 'binary_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9IMgux2KGV_k",
    "outputId": "950023ba-3af3-4168-fc8d-2ddfd7d2aea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1/1 [==============================] - 65s 65s/step - loss: 0.6927 - accuracy: 0.5781 - val_loss: 0.6931 - val_accuracy: 0.5002\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 63s 63s/step - loss: 0.6900 - accuracy: 0.6094 - val_loss: 0.6932 - val_accuracy: 0.4997\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 62s 62s/step - loss: 0.6871 - accuracy: 0.6094 - val_loss: 0.6934 - val_accuracy: 0.4997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40f6ff9190>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "\n",
    "x1 = X_train[:batch_size]\n",
    "y1 = y_train[:batch_size]\n",
    "x_valid = X_train[batch_size:]\n",
    "y_valid = y_train[batch_size:]\n",
    "\n",
    "lstm.fit(\n",
    "    x1, y1,\n",
    "    validation_data = (x_valid, y_valid),\n",
    "    batch_size = batch_size,\n",
    "    epochs = num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBtRY9jmI5VQ"
   },
   "source": [
    "Perform Error analysis and explain using few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrs2ylDaI5VR",
    "outputId": "559cf62d-deab-4fe3-9174-461a0d31ea9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (LSTM):  0.7238239049911499\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Accuracy (LSTM): ', scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlT-aw4rToW7"
   },
   "source": [
    "Running this for more epochs, might yield better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEMV500CHZXE"
   },
   "source": [
    "LSTMS are form of RNN that control the relative influence of the input and prior value of the hidden state. Compared to traditional RNN, LSTM units are able to learn the temporal dependency of the input sequence.\n",
    "\n",
    "There are two components in LSTM unit:\n",
    "* Cell state: cell state is the output of the LSTM and is a function of the current input, the previously hidden state and the previous output\n",
    "* Forget gate: forget gate decides how much the cell state will be influenced by the previous state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf1OvpzuNVce"
   },
   "source": [
    "How to achieve better results?\n",
    "\n",
    "1. Mapping using a simple ID won't be sufficient.\n",
    "1. Consider constructing better embeddings for the words before running the model.\n",
    "2. Consider using more layers.\n",
    "3. Consider stacking multiple layers to capture longer time-series dimensions and also to reduce some of the vanishing gradient problem.\n",
    "4. Consider using an ensemble of smaller neural networks in a stacked architecture to capture the temporal aspect."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
